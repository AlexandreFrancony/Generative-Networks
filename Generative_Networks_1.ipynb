{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Autoencoder on MNIST\n",
    "\n",
    "Exe. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the MNIST dataset from keras.datasets and load it in x train, y train, x test, y test variables.\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#check the train and test shape.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#In oder to be able using the sigmoid activation function, normalize x train and x test according to the maximum and minimum elements of image set, for instance check x train[0].\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some images to see your normalization results.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train[1], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train[2], cmap='gray')\n",
    "plt.show()\n",
    "# Notice The Encoder generally uses a series of Dense and/or Convolutional layers to encode an \n",
    "# image into a fixed length vector that represents the image acompact form, while the Decoder \n",
    "# uses Dense and/or Convolutional layers toconvert the latent representation vector back into \n",
    "# that same image or anothermodified image (see Figure 1).\n",
    "\n",
    "# Latent size is the size of the latent space: the vector holding the information\n",
    "# after compression. This value is a crucial hyperparameter. If this value is too\n",
    "# small, there won’t be enough data for reconstruction and if the value is too\n",
    "# large, overfitting can occur."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s define the LATENT SIZE = 32. Create an encoder model consists of a series  \n",
    "# of dense layers, each layer is followed by a Dropout and a ReLU layer.\n",
    "Latent_Size = 32\n",
    "from keras.layers import Dense, Flatten, LeakyReLU, ReLU, Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "encoder = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(512),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(Latent_Size),\n",
    "        ReLU(),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decoder model namely decoder. \n",
    "# The decoder is essentially the same as the encoder but in reverse.\n",
    "# Dense, ReLU, Dropout, Dense, ReLU, Dropout, Dense, ReLU,\n",
    "# Dropout, Dense, ReLU, Dropout, Dense, Acivation, Reshape\n",
    "decoder = Sequential(\n",
    "    [\n",
    "        Dense(64, input_shape=(Latent_Size,)),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(784),\n",
    "        Activation(\"sigmoid\"),\n",
    "        Reshape((28, 28)),\n",
    "    ])\n",
    "decoder.summary()\n",
    "\n",
    "#decoder = Model(encoded, Flatten()(Dense(64)(LeakyReLU()(Dropout(0.3)(Dense(128)(LeakyReLU()(Dropout(0.3)(Dense(256)(LeakyReLU()(Dropout(0.3)(Dense(512)(LeakyReLU()(Dropout(0.3)(Dense(784)(LeakyReLU()(Dropout(0.3)(encoded)))))))))))))))))\n",
    "#decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the following code in your project:\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img = Input(shape = (28 , 28))\n",
    "latent_vector = encoder(img)\n",
    "output = decoder(latent_vector)\n",
    "model = Model(inputs = img, outputs = output)\n",
    "model.compile(\"nadam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fig, axs = plt.subplots(4, 4)\n",
    "    rand = X_test[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28, 28))\n",
    "    \n",
    "    display.clear_output() # If you imported display from IPython\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(model.predict(rand[i, j])[0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()\n",
    "    print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    "    model.fit(X_train, X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoder on MNIST\n",
    "\n",
    "Exe. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate corrupted MNIST images by adding noise with normal distribution \n",
    "# (mean = 0.5 and std= 0.5) to your x train and x test dataset. Fix\n",
    "# the random seed with your student number.\n",
    "import numpy as np\n",
    "np.random.seed(2017011)\n",
    "noise_factor = 0.5\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding the random generated noises to the x sets, keep only those among 0 and 1 using np.clip()/.\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some of your noisy images to see how they are noisy now.\n",
    "plt.imshow(X_train_noisy[0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train_noisy[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new noisy data with the previous model. How are the\n",
    "# results? How they are close to the real images?\n",
    "# check the noisy data with the previous model\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fig, axs = plt.subplots(4, 4)\n",
    "    rand = X_test_noisy[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28, 28))\n",
    "    \n",
    "    display.clear_output() # If you imported display from IPython\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(model.predict(rand[i, j])[0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()\n",
    "    print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    "    model.fit(X_train_noisy, X_train_noisy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
