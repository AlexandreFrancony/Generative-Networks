{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Autoencoder on MNIST\n",
    "\n",
    "Exe. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the MNIST dataset from keras.datasets and load it in x train, y train, x test, y test variables.\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#check the train and test shape.\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "#In oder to be able using the sigmoid activation function, normalize x train and x test according to the maximum and minimum elements of image set, for instance check x train[0].\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some images to see your normalization results.\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train[1], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train[2], cmap='gray')\n",
    "plt.show()\n",
    "# Notice The Encoder generally uses a series of Dense and/or Convolutional layers to encode an \n",
    "# image into a fixed length vector that represents the image acompact form, while the Decoder \n",
    "# uses Dense and/or Convolutional layers toconvert the latent representation vector back into \n",
    "# that same image or anothermodified image (see Figure 1).\n",
    "\n",
    "# Latent size is the size of the latent space: the vector holding the information\n",
    "# after compression. This value is a crucial hyperparameter. If this value is too\n",
    "# small, there won’t be enough data for reconstruction and if the value is too\n",
    "# large, overfitting can occur."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s define the LATENT SIZE = 32. Create an encoder model consists of a series  \n",
    "# of dense layers, each layer is followed by a Dropout and a ReLU layer.\n",
    "Latent_Size = 32\n",
    "from keras.layers import Dense, Flatten, LeakyReLU, ReLU, Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "encoder = Sequential(\n",
    "    [\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(512),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(Latent_Size),\n",
    "        ReLU(),\n",
    "    ])\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decoder model namely decoder. \n",
    "# The decoder is essentially the same as the encoder but in reverse.\n",
    "# Dense, ReLU, Dropout, Dense, ReLU, Dropout, Dense, ReLU,\n",
    "# Dropout, Dense, ReLU, Dropout, Dense, Acivation, Reshape\n",
    "\n",
    "Latent_Size = 32\n",
    "from keras.layers import Dense, Flatten, LeakyReLU, ReLU, Activation, Dropout, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "decoder = Sequential(\n",
    "    [\n",
    "        Dense(64, input_shape=(Latent_Size,)),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512),\n",
    "        ReLU(),\n",
    "        Dropout(0.5),\n",
    "        Dense(784),\n",
    "        Activation(\"sigmoid\"),\n",
    "        Reshape((28, 28)),\n",
    "    ])\n",
    "decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the following code in your project:\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img = Input(shape = (28 , 28))\n",
    "latent_vector = encoder(img)\n",
    "output = decoder(latent_vector)\n",
    "model = Model(inputs = img, outputs = output)\n",
    "model.compile(\"nadam\", loss = \"binary_crossentropy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "import numpy as np\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fig, axs = plt.subplots(4, 4)\n",
    "    rand = X_test[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28, 28))\n",
    "    \n",
    "    display.clear_output() # If you imported display from IPython\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(model.predict(rand[i, j])[0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()\n",
    "    print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    "    model.fit(X_train, X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising autoencoder on MNIST\n",
    "\n",
    "Exe. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate corrupted MNIST images by adding noise with normal distribution \n",
    "# (mean = 0.5 and std= 0.5) to your x train and x test dataset. Fix\n",
    "# the random seed with your student number.\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=X_train.shape)\n",
    "X_train_noisy = X_train + noise\n",
    "noise = np.random.normal(loc=0.5, scale=0.5, size=X_test.shape)\n",
    "X_test_noisy = X_test + noise\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding the random generated noises to the x sets, keep only those among 0 and 1 using np.clip()/.\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print some of your noisy images to see how they are noisy now.\n",
    "plt.imshow(X_train_noisy[0], cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train_noisy[1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new noisy data with the previous model. How are the\n",
    "# results? How they are close to the real images?\n",
    "# check the noisy data with the previous model\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    fig, axs = plt.subplots(4, 4)\n",
    "    rand = X_test_noisy[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28, 28))\n",
    "    \n",
    "    display.clear_output() # If you imported display from IPython\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(model.predict(rand[i, j])[0], cmap = \"gray\")\n",
    "            axs[i, j].axis(\"off\")\n",
    "    \n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    "    plt.show()\n",
    "    print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    "    model.fit(X_train_noisy, X_train_noisy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time design the encoder using Conv2D networks. The model is\n",
    "# as following:\n",
    "# Input(Inputshape), Conv2d(32), Conv2d(64), Flatten, Dense(Latent vector size)\n",
    "# Define a latent vector as 16, conv2d values with filters 32 and 64 respectively.\n",
    "# While each conv2d has a kernel size = 3, strides 2,activation of relu and\n",
    "# ’padding’ of ’same’.\n",
    "\n",
    "# encoder\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "encoder = Sequential(\n",
    "    [\n",
    "        Input(shape = (28, 28, 1)),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "        Flatten(),\n",
    "        Dense(16),\n",
    "    ])\n",
    "\n",
    "encoder.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build decoder model, based on the following model:\n",
    "# Inout(latent size), Dense(28 ∗28 ∗1), Conv2DT ranspose(64),Conv2DT ranspose(32), Conv2DT ranspose, Activation(′sigmoid′)\n",
    "#decoder : \n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "#Reshape((28, 28, 1))\n",
    "decoder = Sequential(\n",
    "    [\n",
    "        Dense(28*28*1, input_shape=(16,)),\n",
    "        Reshape((28, 28, 1)),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(1, (3, 3), activation='sigmoid', padding='same'),\n",
    "    ])\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the final model including the encoder and decoder models, such as\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "autoencoder = Model(inputs = img, outputs = output, name = \"autoencoder\")\n",
    "autoencoder.summary ()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model using two classical ’mse’ loss function and ’adam’ optimiser\n",
    "autoencoder.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the autoencoder. Notice that the to be trained data\n",
    "# here is x train noisy, while the exact data is x train. It is the same for the\n",
    "# test data set:\n",
    "autoencoder.fit(X_train_noisy ,\n",
    "    X_train ,\n",
    "    validation_data =( X_test_noisy , X_test),\n",
    "    epochs =5,\n",
    "    batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the x test noisy using the trained autoencoder model.\n",
    "fig, axs = plt.subplots(3, 16, figsize=(16, 3))\n",
    "fig.suptitle(\"AE: Predicted images (top), X_test_noisy(middle), X_test (bottom)\")\n",
    "for i in range(16):\n",
    "    axs[0, i].imshow(X_test_noisy[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(autoencoder.predict(X_test_noisy[i].reshape(1, 28, 28, 1)).reshape(28, 28), cmap = \"gray\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "    axs[2, i].imshow(X_test[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[2, i].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some of the x test, x test noisy and predicted x test noisy from the trained model and show compare them in different figures.\n",
    "# You can use the following code to show the results:\n",
    "fig, axs = plt.subplots(3, 16, figsize=(16, 3))\n",
    "fig.suptitle(\"AE: Predicted images (top), X_test_noisy(middle), X_test (bottom)\")\n",
    "for i in range(16):\n",
    "    axs[0, i].imshow(X_test_noisy[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(autoencoder.predict(X_test_noisy[i].reshape(1, 28, 28, 1)).reshape(28, 28), cmap = \"gray\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "    axs[2, i].imshow(X_test[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[2, i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an encoder decoder on Labeled Faces in the Wild datase\n",
    "# http://vis-www.cs.umass.edu/lfw/\n",
    "# Download the dataset and extract it in the same folder as your notebook.\n",
    "# Use the following code to load the dataset:\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "print(lfw_people.DESCR)\n",
    "\n",
    "fig, axs = plt.subplots(3, 16, figsize=(16, 3))\n",
    "fig.suptitle(\"AE: Predicted images (top), X_test_noisy(middle), X_test (bottom)\")\n",
    "for i in range(16):\n",
    "    axs[0, i].imshow(X_test_noisy[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(autoencoder.predict(X_test_noisy[i].reshape(1, 28, 28, 1)).reshape(28, 28), cmap = \"gray\")\n",
    "    axs[1, i].axis(\"off\")\n",
    "    axs[2, i].imshow(X_test[i].reshape(28, 28), cmap = \"gray\")\n",
    "    axs[2, i].axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
