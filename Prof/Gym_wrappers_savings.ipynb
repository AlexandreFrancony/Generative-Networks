{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_gym_wrappers_saving_loading.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/2_gym_wrappers_saving_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"3ezJ3Y7XRUnj"},"source":["# Stable Baselines3 Tutorial - Gym wrappers, saving and loading models\n","\n","Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines3.readthedocs.io/en/master/\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","\n","## Introduction\n","\n","Very frequently, you will want to extend the environement's functionality in some generic way. For example, do some manipulations on the observations before giving them to the agent. Gym provides you with a convenient framework for these situations called the Wrapper class. \n","\n","Another class you should be aware of is Monitor. It is implemented like Wrapper and can write information about your agent's performance in a file with an optional video recording of your agent in action.\n","\n","(Taken from the book Deep RL Maxime Lapan).\n","\n","## Install Dependencies and Stable Baselines3 Using Pip"]},{"cell_type":"code","metadata":{"id":"YFdlFByORUnl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279634893,"user_tz":-60,"elapsed":12300,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"1f30e960-3451-437a-f5da-25be5eb8fe3f"},"source":["!apt install swig\n","!pip install stable-baselines3[extra]"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  swig3.0\n","Suggested packages:\n","  swig-doc swig-examples swig3.0-examples swig3.0-doc\n","The following NEW packages will be installed:\n","  swig swig3.0\n","0 upgraded, 2 newly installed, 0 to remove and 37 not upgraded.\n","Need to get 1,100 kB of archives.\n","After this operation, 5,822 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n","Fetched 1,100 kB in 1s (1,263 kB/s)\n","Selecting previously unselected package swig3.0.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n","Unpacking swig3.0 (3.0.12-1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n","Unpacking swig (3.0.12-1) ...\n","Setting up swig3.0 (3.0.12-1) ...\n","Setting up swig (3.0.12-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting stable-baselines3[extra]\n","  Downloading stable_baselines3-1.3.0-py3-none-any.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.10.0+cu111)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.2.9)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.7.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->stable-baselines3[extra]) (1.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.4.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable-baselines3[extra]) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->stable-baselines3[extra]) (0.16.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.12.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.42.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.23.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->stable-baselines3[extra]) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.1.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n","Installing collected packages: stable-baselines3\n","Successfully installed stable-baselines3-1.3.0\n"]}]},{"cell_type":"code","metadata":{"id":"grXe85G9RUnp","executionInfo":{"status":"ok","timestamp":1638279648663,"user_tz":-60,"elapsed":6483,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["import gym\n","from stable_baselines3 import A2C, SAC, PPO, TD3"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hMPAn1SRd32f"},"source":["# Saving and loading\n","\n","Saving and loading stable-baselines models is straightforward: you can directly call `.save()` and `.load()` on the models."]},{"cell_type":"code","metadata":{"id":"vBNFnN4Gd32g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279663450,"user_tz":-60,"elapsed":10747,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"9cbfcc19-d250-4653-b283-8ab4760abc23"},"source":["import os\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\" #look at this directory\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = PPO('MlpPolicy', 'Pendulum-v0', verbose=0).learn(8000)\n","# The model will be saved under PPO_tutorial.zip\n","model.save(save_dir + \"/PPO_tutorial\")\n","\n","# sample an observation from the environment\n","obs = model.env.observation_space.sample()\n","\n","# Check prediction before saving\n","print(\"pre saved\", model.predict(obs, deterministic=True))\n","\n","del model # delete trained model to demonstrate loading\n","\n","loaded_model = PPO.load(save_dir + \"/PPO_tutorial\")\n","# Check that the prediction is the same after loading (for the same observation)\n","print(\"loaded\", loaded_model.predict(obs, deterministic=True))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["pre saved (array([-0.04128068], dtype=float32), None)\n","loaded (array([-0.04128068], dtype=float32), None)\n"]}]},{"cell_type":"markdown","metadata":{"id":"gXWPrVqId32o"},"source":["Saving in stable-baselines is quite powerful, as you save the training hyperparameters, with the current weights. This means in practice, you can simply load a custom model, without redefining the parameters, and continue learning.\n","\n","The loading function can also update the model's class variables when loading.\n","Here, we do with another training method A2C.\n","See this page for Vectorized Environments: https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html"]},{"cell_type":"code","metadata":{"id":"LCtxrAbXd32q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279754576,"user_tz":-60,"elapsed":15666,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"3ea7e4e3-1036-4b93-bfa3-56a5da6bb1ed"},"source":["import os\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","\n","# Create save dir\n","save_dir = \"/tmp/gym/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","model = A2C('MlpPolicy', 'Pendulum-v0', verbose=0, gamma=0.9, n_steps=20).learn(8000)\n","# The model will be saved under A2C_tutorial.zip\n","model.save(save_dir + \"/A2C_tutorial\")\n","\n","del model # delete trained model to demonstrate loading\n","\n","# load the model, and when loading set verbose to 1\n","loaded_model = A2C.load(save_dir + \"/A2C_tutorial\", verbose=1)\n","\n","# show the save hyperparameters\n","print(\"loaded:\", \"gamma =\", loaded_model.gamma, \"n_steps =\", loaded_model.n_steps)\n","\n","# as the environment is not serializable, we need to set a new instance of the environment with DummyVecEnv\n","#Équivalent à une exécution en série, pour un processus\n","#Here, we continue learning\n","loaded_model.set_env(DummyVecEnv([lambda: gym.make('Pendulum-v0')]))\n","# and continue training\n","loaded_model.learn(8000)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["loaded: gamma = 0.9 n_steps = 20\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 1030     |\n","|    iterations         | 100      |\n","|    time_elapsed       | 1        |\n","|    total_timesteps    | 2000     |\n","| train/                |          |\n","|    entropy_loss       | -1.47    |\n","|    explained_variance | 0.0108   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 499      |\n","|    policy_loss        | -43.9    |\n","|    std                | 1.05     |\n","|    value_loss         | 1.12e+03 |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 1039     |\n","|    iterations         | 200      |\n","|    time_elapsed       | 3        |\n","|    total_timesteps    | 4000     |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.0221   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 599      |\n","|    policy_loss        | -15.3    |\n","|    std                | 1.04     |\n","|    value_loss         | 205      |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 1039     |\n","|    iterations         | 300      |\n","|    time_elapsed       | 5        |\n","|    total_timesteps    | 6000     |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.0212   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 699      |\n","|    policy_loss        | -22.1    |\n","|    std                | 1.04     |\n","|    value_loss         | 677      |\n","------------------------------------\n","------------------------------------\n","| time/                 |          |\n","|    fps                | 1046     |\n","|    iterations         | 400      |\n","|    time_elapsed       | 7        |\n","|    total_timesteps    | 8000     |\n","| train/                |          |\n","|    entropy_loss       | -1.46    |\n","|    explained_variance | 0.0671   |\n","|    learning_rate      | 0.0007   |\n","|    n_updates          | 799      |\n","|    policy_loss        | -17.4    |\n","|    std                | 1.05     |\n","|    value_loss         | 501      |\n","------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.a2c.a2c.A2C at 0x7f167c47f8d0>"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"hKwupU-Jgxjm"},"source":["# Gym and VecEnv wrappers"]},{"cell_type":"markdown","metadata":{"id":"ds4AAfmISQIA"},"source":["## Anatomy of a gym wrapper"]},{"cell_type":"markdown","metadata":{"id":"gnTS9e9hTzZZ"},"source":["A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n","\n","Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n","There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"]},{"cell_type":"markdown","metadata":{"id":"4zeGuyICUN26"},"source":["## First example: limit the episode length\n","\n","One practical use case of a wrapper is when you want to limit the number of steps by episode, for that you will need to overwrite the `done` signal when the limit is reached. It is also a good practice to pass that information in the `info` dictionnary."]},{"cell_type":"code","metadata":{"id":"Eb2U4_K6SNUx","executionInfo":{"status":"ok","timestamp":1638279765960,"user_tz":-60,"elapsed":289,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["class TimeLimitWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  :param max_steps: (int) Max number of steps per episode\n","  \"\"\"\n","  def __init__(self, env, max_steps=100):\n","    # Call the parent constructor, so we can access self.env later\n","    super(TimeLimitWrapper, self).__init__(env)\n","    self.max_steps = max_steps\n","    # Counter of steps per episode\n","    self.current_step = 0\n","  \n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    # Reset the counter\n","    self.current_step = 0\n","    return self.env.reset()\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    self.current_step += 1\n","    obs, reward, done, info = self.env.step(action)\n","    # Overwrite the done signal when \n","    if self.current_step >= self.max_steps:\n","      done = True\n","      # Update the info dict to signal that the limit was exceeded\n","      info['time_limit_reached'] = True\n","    return obs, reward, done, info\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZufaUJwVM9w"},"source":["#### Test the wrapper"]},{"cell_type":"code","metadata":{"id":"szZ43D5PVB07","executionInfo":{"status":"ok","timestamp":1638279771873,"user_tz":-60,"elapsed":237,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["from gym.envs.classic_control.pendulum import PendulumEnv\n","\n","# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n","env = PendulumEnv()\n","# Wrap the environment\n","env = TimeLimitWrapper(env, max_steps=100)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"cencka9iVg9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279776093,"user_tz":-60,"elapsed":218,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"b51ed947-3830-4c75-f6d7-0a79307a6d06"},"source":["obs = env.reset()\n","done = False\n","n_steps = 0\n","while not done:\n","  # Take random actions\n","  random_action = env.action_space.sample()\n","  obs, reward, done, info = env.step(random_action)\n","  n_steps += 1\n","\n","print(n_steps, info)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["100 {'time_limit_reached': True}\n"]}]},{"cell_type":"markdown","metadata":{"id":"jkMYA63sV9aA"},"source":["In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."]},{"cell_type":"markdown","metadata":{"id":"VIIJbSyQW9R-"},"source":["## Second example: normalize actions\n","\n","It is usually a good idea to normalize observations and actions before giving it to the agent, this prevent [hard to debug issue](https://github.com/hill-a/stable-baselines/issues/473).\n","\n","In this example, we are going to normalize the action space of *Pendulum-v0* so it lies in [-1, 1] instead of [-2, 2]."]},{"cell_type":"code","metadata":{"id":"F5E6kZfzW8vy","executionInfo":{"status":"ok","timestamp":1638279781176,"user_tz":-60,"elapsed":216,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["import numpy as np\n","\n","class NormalizeActionWrapper(gym.Wrapper):\n","  \"\"\"\n","  :param env: (gym.Env) Gym environment that will be wrapped\n","  \"\"\"\n","  def __init__(self, env):\n","    # Retrieve the action space\n","    action_space = env.action_space\n","    assert isinstance(action_space, gym.spaces.Box), \"This wrapper only works with continuous action space (spaces.Box)\"\n","    # Retrieve the max/min values\n","    self.low, self.high = action_space.low, action_space.high\n","\n","    # We modify the action space, so all actions will lie in [-1, 1]\n","    env.action_space = gym.spaces.Box(low=-1, high=1, shape=action_space.shape, dtype=np.float32)\n","\n","    # Call the parent constructor, so we can access self.env later\n","    super(NormalizeActionWrapper, self).__init__(env)\n","  \n","  def rescale_action(self, scaled_action):\n","      \"\"\"\n","      Rescale the action from [-1, 1] to [low, high]\n","      (no need for symmetric action space)\n","      :param scaled_action: (np.ndarray)\n","      :return: (np.ndarray)\n","      \"\"\"\n","      return self.low + (0.5 * (scaled_action + 1.0) * (self.high -  self.low))\n","\n","  def reset(self):\n","    \"\"\"\n","    Reset the environment \n","    \"\"\"\n","    # Reset the counter\n","    return self.env.reset()\n","\n","  def step(self, action):\n","    \"\"\"\n","    :param action: ([float] or int) Action taken by the agent\n","    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n","    \"\"\"\n","    # Rescale action from [-1, 1] to original [low, high] interval\n","    rescaled_action = self.rescale_action(action)\n","    obs, reward, done, info = self.env.step(rescaled_action)\n","    return obs, reward, done, info\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TmJ0eahNaR6K"},"source":["#### Test before rescaling actions"]},{"cell_type":"code","metadata":{"id":"UEnjBwisaQIx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279786155,"user_tz":-60,"elapsed":227,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"05c51d2b-ee07-4e67-8429-602892e7f679"},"source":["original_env = gym.make(\"Pendulum-v0\")\n","\n","print(original_env.action_space.low)\n","for _ in range(10):\n","  print(original_env.action_space.sample())"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[-2.]\n","[-1.821175]\n","[1.8000855]\n","[-0.72909755]\n","[-1.0894296]\n","[0.02703821]\n","[-0.4260787]\n","[0.8469812]\n","[-0.0250614]\n","[-0.60594916]\n","[-1.5737854]\n"]}]},{"cell_type":"markdown","metadata":{"id":"jvcll2L3afVd"},"source":["#### Test the NormalizeAction wrapper"]},{"cell_type":"code","metadata":{"id":"WsCM9AUGaeBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279794567,"user_tz":-60,"elapsed":232,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"d51d8ebb-3672-47bf-b4bb-84aa8a8fac13"},"source":["env = NormalizeActionWrapper(gym.make(\"Pendulum-v0\"))\n","\n","print(env.action_space.low)\n","\n","for _ in range(10):\n","  print(env.action_space.sample())"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[-1.]\n","[0.26289186]\n","[0.31486264]\n","[0.07406089]\n","[-0.25210303]\n","[0.3383621]\n","[0.04249078]\n","[0.97044873]\n","[-0.30929026]\n","[-0.65102375]\n","[-0.65454125]\n"]}]},{"cell_type":"markdown","metadata":{"id":"V5h5kk2mbGNs"},"source":["#### Test with a RL algorithm\n","\n","We are going to use the Monitor wrapper of stable baselines, wich allow to monitor training stats (mean episode reward, mean episode length)\n","\n","See here for more details: https://stable-baselines3.readthedocs.io/en/master/common/monitor.html"]},{"cell_type":"code","metadata":{"id":"R9FNCN8ybOVU","executionInfo":{"status":"ok","timestamp":1638279831195,"user_tz":-60,"elapsed":224,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import DummyVecEnv"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"wutM3c1GbfGP","executionInfo":{"status":"ok","timestamp":1638279833432,"user_tz":-60,"elapsed":3,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["env = Monitor(gym.make('Pendulum-v0'))\n","env = DummyVecEnv([lambda: env])"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cxnE5bdaQ_3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279838012,"user_tz":-60,"elapsed":1767,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"3e64d3b7-7ed0-4874-a203-0187b1238a3f"},"source":["model = A2C(\"MlpPolicy\", env, verbose=1).learn(int(1000))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.23e+03 |\n","| time/                 |           |\n","|    fps                | 652       |\n","|    iterations         | 100       |\n","|    time_elapsed       | 0         |\n","|    total_timesteps    | 500       |\n","| train/                |           |\n","|    entropy_loss       | -1.42     |\n","|    explained_variance | -0.00511  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 99        |\n","|    policy_loss        | -34       |\n","|    std                | 1         |\n","|    value_loss         | 1.33e+03  |\n","-------------------------------------\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.21e+03 |\n","| time/                 |           |\n","|    fps                | 648       |\n","|    iterations         | 200       |\n","|    time_elapsed       | 1         |\n","|    total_timesteps    | 1000      |\n","| train/                |           |\n","|    entropy_loss       | -1.42     |\n","|    explained_variance | -0.0354   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 199       |\n","|    policy_loss        | -30.5     |\n","|    std                | 0.998     |\n","|    value_loss         | 599       |\n","-------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"EJFSM-Drb3Wc"},"source":["With the action wrapper"]},{"cell_type":"code","metadata":{"id":"GszFZthob2wM","executionInfo":{"status":"ok","timestamp":1638279842599,"user_tz":-60,"elapsed":227,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}}},"source":["normalized_env = Monitor(gym.make('Pendulum-v0'))\n","# Note that we can use multiple wrappers\n","normalized_env = NormalizeActionWrapper(normalized_env)\n","normalized_env = DummyVecEnv([lambda: normalized_env])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"wrKJEO4NcIMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638279847316,"user_tz":-60,"elapsed":1768,"user":{"displayName":"Hatem Hajri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhARW7xuumGEfUIfKBTHqCbuEzHHtoNPfwAAcCHPg=s64","userId":"03506239018212584198"}},"outputId":"114175e4-85a7-48a5-89e0-45a0fc38e84d"},"source":["model_2 = A2C(\"MlpPolicy\", normalized_env, verbose=1).learn(int(1000))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.25e+03 |\n","| time/                 |           |\n","|    fps                | 625       |\n","|    iterations         | 100       |\n","|    time_elapsed       | 0         |\n","|    total_timesteps    | 500       |\n","| train/                |           |\n","|    entropy_loss       | -1.43     |\n","|    explained_variance | -0.0204   |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 99        |\n","|    policy_loss        | -35.7     |\n","|    std                | 1.01      |\n","|    value_loss         | 1.39e+03  |\n","-------------------------------------\n","-------------------------------------\n","| rollout/              |           |\n","|    ep_len_mean        | 200       |\n","|    ep_rew_mean        | -1.28e+03 |\n","| time/                 |           |\n","|    fps                | 631       |\n","|    iterations         | 200       |\n","|    time_elapsed       | 1         |\n","|    total_timesteps    | 1000      |\n","| train/                |           |\n","|    entropy_loss       | -1.44     |\n","|    explained_variance | 0.000875  |\n","|    learning_rate      | 0.0007    |\n","|    n_updates          | 199       |\n","|    policy_loss        | -57       |\n","|    std                | 1.02      |\n","|    value_loss         | 1.91e+03  |\n","-------------------------------------\n"]}]}]}