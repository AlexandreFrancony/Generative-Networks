{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb","timestamp":1636724068163}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hyyN-2qyK_T2"},"source":["# Stable Baselines3 Tutorial - Getting Started\n","\n","Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n","\n","Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n","\n","Documentation: https://stable-baselines.readthedocs.io/en/master/\n","\n","RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n","\n","\n","[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n","\n","It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n","\n","\n","## Introduction\n","\n","In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n","\n","\n","## Install Dependencies and Stable Baselines3 Using Pip\n","\n","List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n","\n","\n","```\n","pip install stable-baselines3[extra]\n","```"]},{"cell_type":"code","metadata":{"id":"gWskDE2c9WoN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670146585184,"user_tz":-60,"elapsed":37838,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"05417cb8-bb77-45ac-9c50-8735df4fc6e8"},"source":["!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","!pip install stable-baselines3[extra]"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following NEW packages will be installed:\n","  freeglut3 freeglut3-dev xvfb\n","0 upgraded, 3 newly installed, 0 to remove and 7 not upgraded.\n","Need to get 982 kB of archives.\n","After this operation, 3,350 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 freeglut3-dev amd64 2.8.1-3 [124 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.12 [785 kB]\n","Fetched 982 kB in 0s (7,864 kB/s)\n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 124015 files and directories currently installed.)\n","Preparing to unpack .../freeglut3_2.8.1-3_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-3) ...\n","Selecting previously unselected package freeglut3-dev:amd64.\n","Preparing to unpack .../freeglut3-dev_2.8.1-3_amd64.deb ...\n","Unpacking freeglut3-dev:amd64 (2.8.1-3) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.12_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.12) ...\n","Setting up freeglut3:amd64 (2.8.1-3) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.12) ...\n","Setting up freeglut3-dev:amd64 (2.8.1-3) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting stable-baselines3[extra]\n","  Downloading stable_baselines3-1.6.2-py3-none-any.whl (170 kB)\n","\u001b[K     |████████████████████████████████| 170 kB 13.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.12.1+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.21.6)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.5.0)\n","Collecting gym==0.21\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 58.0 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (1.3.5)\n","Requirement already satisfied: importlib-metadata~=4.13 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.13.0)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (2.9.1)\n","Collecting ale-py==0.7.4\n","  Downloading ale_py-0.7.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 56.1 MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (5.4.8)\n","Collecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting rich\n","  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n","\u001b[K     |████████████████████████████████| 237 kB 63.9 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.6.0.66)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (4.64.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from stable-baselines3[extra]) (7.1.2)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py==0.7.4->stable-baselines3[extra]) (5.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata~=4.13->stable-baselines3[extra]) (3.10.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.14.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.38.4)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.8.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.19.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.50.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (57.4.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.11->stable-baselines3[extra]) (4.1.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable-baselines3[extra]) (2022.6)\n","Collecting commonmark<0.10.0,>=0.9.0\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n","\u001b[K     |████████████████████████████████| 51 kB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich->stable-baselines3[extra]) (2.6.1)\n","Building wheels for collected packages: gym, AutoROM.accept-rom-license\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616825 sha256=0be240d6c98a763029e5ca68fe5eaa153fa410a466c4fa2cc3d845c871d7e855\n","  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441028 sha256=7f0e01c812246e22349b279f6d093c36463664c502c46be7a8a2cf20fb8f96bd\n","  Stored in directory: /root/.cache/pip/wheels/51/08/c5/28b973078691a3f8baf99fcaec1ed8f0e05ef6e54d2390212c\n","Successfully built gym AutoROM.accept-rom-license\n","Installing collected packages: gym, commonmark, AutoROM.accept-rom-license, autorom, stable-baselines3, rich, ale-py\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.4 autorom-0.4.2 commonmark-0.9.1 gym-0.21.0 rich-12.6.0 stable-baselines3-1.6.2\n"]}]},{"cell_type":"code","metadata":{"id":"U29X1-B-AIKE","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1670146588617,"user_tz":-60,"elapsed":3442,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"1e202fcc-4974-419c-dd8e-632934c93c5e"},"source":["import stable_baselines3\n","stable_baselines3.__version__"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.6.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"FtY8FhliLsGm"},"source":["## Imports"]},{"cell_type":"markdown","metadata":{"id":"gcX8hEcaUpR0"},"source":["Stable-Baselines works on environments that follow the [gym interface](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n","You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n","\n","It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n","Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)"]},{"cell_type":"code","metadata":{"id":"BIedd7Pz9sOs","executionInfo":{"status":"ok","timestamp":1670146588618,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["import gym\n","import numpy as np"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ae32CtgzTG3R"},"source":["The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"]},{"cell_type":"code","metadata":{"id":"R7tKaBFrTR0a","executionInfo":{"status":"ok","timestamp":1670146588618,"user_tz":-60,"elapsed":15,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["from stable_baselines3 import PPO"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0_8OQbOTTNT"},"source":["The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n","This step is optional as you can directly use strings in the constructor: \n","\n","```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n","\n","Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."]},{"cell_type":"markdown","metadata":{"id":"zGndYBRKYVUJ"},"source":["MlpPolicy is a Policy object that implements actor critic, using a MLP (2 layers of 64)"]},{"cell_type":"code","metadata":{"id":"ROUJr675TT01","executionInfo":{"status":"ok","timestamp":1670146588619,"user_tz":-60,"elapsed":14,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["from stable_baselines3.ppo import MlpPolicy"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FwAXVS86Y7VO"},"source":["#The CartePole environment. \n","Description by gym: https://gym.openai.com/envs/CartPole-v1/\n","A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n","The observations given to the agents at each time are 4: \n","**Cart Position, Cart Velocity, Pole angle, Pole Angular Velocity**.\n","See more in the [gym code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py).\n","\n","A nice video illustration is [here](https://www.youtube.com/watch?v=J7E6_my3CHk). "]},{"cell_type":"markdown","metadata":{"id":"RapkYvTXL7Cd"},"source":["## Create the Gym env and instantiate the agent\n","\n","For this example, we will use CartPole environment, a classic control problem.\n","\n","\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n","\n","Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n","\n","![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n","\n","Here we are using the [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n","\n","It combines ideas from [A2C](https://stable-baselines.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n","\n","PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n","It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"]},{"cell_type":"code","metadata":{"id":"pUWGZp3i9wyf","executionInfo":{"status":"ok","timestamp":1670146593493,"user_tz":-60,"elapsed":4887,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["env = gym.make('CartPole-v1')\n","\n","model = PPO(MlpPolicy, env, verbose=0)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1sKYhKSBZ3nU"},"source":["verbose =0 or 1 related to show or not the details during training."]},{"cell_type":"markdown","metadata":{"id":"4efFdrQ7MBvl"},"source":["We create a helper function to evaluate the agent:"]},{"cell_type":"code","metadata":{"id":"63M8mSKR-6Zt","executionInfo":{"status":"ok","timestamp":1670146593494,"user_tz":-60,"elapsed":10,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["def evaluate(model, num_episodes=100, deterministic=True):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param model: (BaseRLModel object) the RL Agent\n","    :param num_episodes: (int) number of episodes to evaluate it\n","    :return: (float) Mean reward for the last num_episodes\n","    \"\"\"\n","    # This function will only work for a single Environment\n","    env = model.get_env()\n","    all_episode_rewards = []\n","    for i in range(num_episodes):\n","        episode_rewards = []\n","        done = False\n","        obs = env.reset()\n","        while not done:\n","            # _states are only useful when using LSTM policies\n","            action, _states = model.predict(obs, deterministic=deterministic)\n","            # here, action, rewards and dones are arrays\n","            # because we are using vectorized env\n","            obs, reward, done, info = env.step(action)\n","            episode_rewards.append(reward)\n","\n","        all_episode_rewards.append(sum(episode_rewards))\n","\n","    mean_episode_reward = np.mean(all_episode_rewards)\n","    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n","\n","    return mean_episode_reward"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hkyafs--gJz"},"source":["In fact, Stable-Baselines3 already provides you with that helper:"]},{"cell_type":"code","metadata":{"id":"s6ZNldIR-fce","executionInfo":{"status":"ok","timestamp":1670146593495,"user_tz":-60,"elapsed":9,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["from stable_baselines3.common.evaluation import evaluate_policy"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjEVOIY8NVeK"},"source":["Let's evaluate the un-trained agent, this should be a random agent."]},{"cell_type":"code","metadata":{"id":"xDHLMA6NFk95","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670146598346,"user_tz":-60,"elapsed":4859,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"efe169ff-3eba-4c91-99fb-0fdd88e38032"},"source":["# Use a separate environement for evaluation\n","eval_env = gym.make('CartPole-v1')\n","\n","# Random Agent, before training\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["mean_reward:9.25 +/- 0.71\n"]}]},{"cell_type":"markdown","metadata":{"id":"r5UoXTZPNdFE"},"source":["## Train the agent and evaluate it"]},{"cell_type":"code","metadata":{"id":"e4cfSXIB-pTF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670146617471,"user_tz":-60,"elapsed":19136,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"de86b181-350e-486f-9fdb-6deaeb4a7380"},"source":["# Train the agent for 10000 steps\n","model.learn(total_timesteps=10000)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.ppo.ppo.PPO at 0x7f93469a6190>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ygl_gVmV_QP7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670146644266,"user_tz":-60,"elapsed":26813,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"9e90fb34-a0dd-459b-f607-c7743002a368"},"source":["# Evaluate the trained agent\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["mean_reward:350.68 +/- 107.82\n"]}]},{"cell_type":"markdown","metadata":{"id":"A00W6yY3NkHG"},"source":["\n","\n","```\n","# Ce texte est au format code\n","```\n","\n","Apparently the training went well, the mean reward increased a lot ! "]},{"cell_type":"markdown","metadata":{"id":"xVm9QPNVwKXN"},"source":["### Prepare video recording"]},{"cell_type":"code","metadata":{"id":"MPyfQxD5z26J","executionInfo":{"status":"ok","timestamp":1670146644267,"user_tz":-60,"elapsed":45,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["# Set up fake display; otherwise rendering will fail\n","import os\n","os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n","os.environ['DISPLAY'] = ':1'"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLzXxO8VMD6N","executionInfo":{"status":"ok","timestamp":1670146644271,"user_tz":-60,"elapsed":48,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["import base64\n","from pathlib import Path\n","\n","from IPython import display as ipythondisplay\n","\n","def show_videos(video_path='', prefix=''):\n","  \"\"\"\n","  Taken from https://github.com/eleurent/highway-env\n","\n","  :param video_path: (str) Path to the folder containing videos\n","  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n","  \"\"\"\n","  html = []\n","  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n","      video_b64 = base64.b64encode(mp4.read_bytes())\n","      html.append('''<video alt=\"{}\" autoplay \n","                    loop controls style=\"height: 400px;\">\n","                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n","                </video>'''.format(mp4, video_b64.decode('ascii')))\n","  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LTRNUfulOGaF"},"source":["We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook.\n","\n","Visit this page to understand the difference between DummyVecEnv and SubprocVecEnv: https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html"]},{"cell_type":"code","metadata":{"id":"Trag9dQpOIhx","executionInfo":{"status":"ok","timestamp":1670146644273,"user_tz":-60,"elapsed":48,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n","\n","def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n","  \"\"\"\n","  :param env_id: (str)\n","  :param model: (RL model)\n","  :param video_length: (int)\n","  :param prefix: (str)\n","  :param video_folder: (str)\n","  \"\"\"\n","  eval_env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n","  # Start the video at step=0 and record 500 steps\n","  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n","                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n","                              name_prefix=prefix)\n","\n","  obs = eval_env.reset()\n","  for _ in range(video_length):\n","    action, _ = model.predict(obs)\n","    obs, _, _, _ = eval_env.step(action)\n","\n","  # Close the video recorder\n","  eval_env.close()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOObbeu5MMlR"},"source":["### Visualize trained agent\n","\n"]},{"cell_type":"code","metadata":{"id":"iATu7AiyMQW2","colab":{"base_uri":"https://localhost:8080/","height":658},"executionInfo":{"status":"error","timestamp":1670146708808,"user_tz":-60,"elapsed":535,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}},"outputId":"ca4e62e8-a296-49a0-fe52-6cc666ed0fde"},"source":["record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"],"execution_count":16,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyglet'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-e858dbb4fbff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecord_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ppo-cartpole'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-8813247897ce>\u001b[0m in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     15\u001b[0m                               name_prefix=prefix)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvObs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\u001b[0m in \u001b[0;36mstart_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecording\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ansi\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \"\"\"\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0mCannot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: \n    Cannot import pyglet.\n    HINT: you can install pyglet directly via 'pip install pyglet'.\n    But if you really just want to install all Gym dependencies and not have to think about it,\n    'pip install -e .[all]' or 'pip install gym[all]' will do it.\n    ","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"-n4i-fW3NojZ","executionInfo":{"status":"aborted","timestamp":1670146644738,"user_tz":-60,"elapsed":13,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["show_videos('videos', prefix='ppo')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Y8zg4V566qD"},"source":["## Bonus: Train a RL Model in One Line\n","\n","The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html)."]},{"cell_type":"code","metadata":{"id":"iaOPfOrwWEP4","executionInfo":{"status":"aborted","timestamp":1670146644739,"user_tz":-60,"elapsed":14,"user":{"displayName":"Hatem Hajri","userId":"03506239018212584198"}}},"source":["model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FrI6f5fWnzp-"},"source":["## Conclusion\n","\n","In this notebook we have seen:\n","- how to define and train a RL model using stable baselines3, it takes only one line of code ;)\n"]}]}