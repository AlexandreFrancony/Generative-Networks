{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short Term Memory (LSTM)\n",
    "\n",
    "Exe. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the price data set, data.csv using pandas. Check the uploaded dataset and its different columns\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "prices_dataset = pd.read_csv('data.csv')\n",
    "\n",
    "prices_dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regarding the uploaded dataset, the ’symbol’ column contains a shortbname of each company.\n",
    "# For instance YHOO represent Yahoo. Get a subsetnof dataset, contaning YHOO symbol. name this subset of data as yahoo. \n",
    "# Get only the high column values (6th column) and save them in a variable namely yahoo stock prices, and demonstrate high values in a graph.\n",
    "yahoo = prices_dataset[prices_dataset['symbol']=='YHOO']\n",
    "print(yahoo.head())\n",
    "yahoo_stock_prices = yahoo.values[:,-2].astype('float32')\n",
    "print(yahoo_stock_prices.shape)\n",
    "yahoo_stock_prices = yahoo_stock_prices.reshape(yahoo_stock_prices.shape[0], 1)\n",
    "print(yahoo_stock_prices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now divide the yahoo stock prices into train and test variables.\n",
    "# For instance 80% for train and 20% for the test set. \n",
    "# Do NOT shuffle the data set because it is a sequential dataset and the its order is important.\n",
    "train_size = int(len(yahoo_stock_prices) * 0.80)\n",
    "test_size = len(yahoo_stock_prices) - train_size\n",
    "train, test = yahoo_stock_prices[0:train_size], yahoo_stock_prices[train_size:len(yahoo_stock_prices)]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network model as such : LSTM, Droupout, LSTM, Droupout, Dense, Activation.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_dim = 1, units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences = False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Compile the model using a mean square error and a rmsprop optimisation.\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fit the model on tainX and trainY datasets while defining your batch size, epochs and validation split.\n",
    "model.fit(trainX, trainY, batch_size=1, epochs=5, validation_split=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the textX with the trained LSTM model and demonstrate the exact and predicted value in a graph. \n",
    "# What is your observation?\n",
    "predicted = model.predict(testX)\n",
    "print(predicted.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(testY)\n",
    "plt.plot(predicted)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train another model on a different company from the original dataset? How is your prediction method?\n",
    "\n",
    "BMY = prices_dataset[prices_dataset['symbol']=='BMY']\n",
    "print(BMY.head())\n",
    "BMY_stock_prices = BMY.values[:,-2].astype('float32')\n",
    "print(BMY_stock_prices.shape)\n",
    "BMY_stock_prices = BMY_stock_prices.reshape(BMY_stock_prices.shape[0], 1)\n",
    "print(BMY_stock_prices)\n",
    "\n",
    "train_size = int(len(BMY_stock_prices) * 0.80)\n",
    "test_size = len(BMY_stock_prices) - train_size\n",
    "train, test = BMY_stock_prices[0:train_size], BMY_stock_prices[train_size:len(BMY_stock_prices)]\n",
    "print(len(train), len(test))\n",
    "\n",
    "trainX, trainY = create_dataset(train, 1)\n",
    "testX, testY = create_dataset(test, 1)\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_dim = 1, units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences = False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1, activation = 'linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "model.fit(trainX, trainY, batch_size=1, epochs=20, validation_split=0.05)\n",
    "\n",
    "predicted = model.predict(testX)\n",
    "print(predicted.shape)\n",
    "print(testY.shape)\n",
    "\n",
    "plt.plot(testY)\n",
    "plt.plot(predicted)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent unit (GRU)\n",
    "\n",
    "Exe. 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Reading the text file into a string \n",
    "with open('poems.txt', 'r') as file: \n",
    "\ttext = file.read() \n",
    "\n",
    "# A preview of the text file\t \n",
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all the unique characters present in the text\n",
    "vocabulary = sorted(list(set(text)))\n",
    "\n",
    "#Creating dictionaries to map each character to an index\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))\n",
    "\n",
    "# A preview of the dictionaries\n",
    "print(vocabulary)\n",
    "print(char_to_indices)\n",
    "print(indices_to_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - max_length, step):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# A preview of the sentences and next_chars\n",
    "print(sentences[0])\n",
    "print(next_chars[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((len(sentences), max_length , len(vocabulary)), dtype = bool)\n",
    "y = np.zeros((len(sentences), len(vocabulary)), dtype = bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_indices[char]] = 1\n",
    "    y[i, char_to_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model you network as GRU (128), Dense(len(vocabulary))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input_shape=(max_length, len(vocabulary))))\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Finally compile the model with categorical cross entropy and a RMSprop optimiser\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the trained model on X and Y datasets by defining a batch size and epochs values. \n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# Check loss and accuracy of your model.\n",
    "score = model.evaluate(X, y, batch_size=128)\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a text similar to the trained data\n",
    "# chose part of the original text randomly and separate a 100 size text from it.\n",
    "start_index = np.random.randint(0, len(text) - max_length - 1)\n",
    "generated_text = text[start_index: start_index + max_length]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "# Generate 100 characters\n",
    "for i in range(100):\n",
    "    sampled = np.zeros((1, max_length, len(vocabulary)))\n",
    "    for t, char in enumerate(generated_text):\n",
    "        sampled[0, t, char_to_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = np.argmax(preds)\n",
    "    next_char = indices_to_char[next_index]\n",
    "\n",
    "    generated_text += next_char\n",
    "    generated_text = generated_text[1:]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exe. 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similar text generator method for another text data source as Sherlock Holmes.\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Reading the text file into a string\n",
    "with open('sherlock.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# A preview of the text file\n",
    "print(text)\n",
    "\n",
    "#storing all the unique characters present in the text\n",
    "vocabulary = sorted(list(set(text)))\n",
    "\n",
    "#Creating dictionaries to map each character to an index\n",
    "char_to_indices = dict((c, i) for i, c in enumerate(vocabulary))\n",
    "indices_to_char = dict((i, c) for i, c in enumerate(vocabulary))\n",
    "\n",
    "# A preview of the dictionaries\n",
    "print(vocabulary)\n",
    "print(char_to_indices)\n",
    "print(indices_to_char)\n",
    "\n",
    "max_length = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - max_length, step):\n",
    "    sentences.append(text[i: i + max_length])\n",
    "    next_chars.append(text[i + max_length])\n",
    "\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# A preview of the sentences and next_chars\n",
    "print(sentences[0])\n",
    "print(next_chars[0])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.zeros((len(sentences), max_length , len(vocabulary)), dtype = bool)\n",
    "y = np.zeros((len(sentences), len(vocabulary)), dtype = bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_to_indices[char]] = 1\n",
    "    y[i, char_to_indices[next_chars[i]]] = 1\n",
    "\n",
    "# model you network as GRU (128), Dense(len(vocabulary))\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(128, input_shape=(max_length, len(vocabulary))))\n",
    "model.add(Dense(len(vocabulary), activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Finally compile the model with categorical cross entropy and a RMSprop optimiser\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# fit the trained model on X and Y datasets by defining a batch size and epochs values.\n",
    "model.fit(X, y, batch_size=128, epochs=100)\n",
    "\n",
    "# Check loss and accuracy of your model.\n",
    "score = model.evaluate(X, y, batch_size=128)\n",
    "print(score)\n",
    "\n",
    "# Generate a text similar to the trained data\n",
    "# chose part of the original text randomly and separate a 100 size text from it.\n",
    "start_index = np.random.randint(0, len(text) - max_length - 1)\n",
    "generated_text = text[start_index: start_index + max_length]\n",
    "print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "# Generate 1000 characters\n",
    "for i in range(1000):\n",
    "    sampled = np.zeros((1, max_length, len(vocabulary)))\n",
    "    for t, char in enumerate(generated_text):\n",
    "        sampled[0, t, char_to_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(sampled, verbose=0)[0]\n",
    "    next_index = np.argmax(preds)\n",
    "    next_char = indices_to_char[next_index]\n",
    "\n",
    "    generated_text += next_char\n",
    "    generated_text = generated_text[1:]\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0d8f4ad52842acb1bd88b5e4d5a3e855ffe6009d6de89734aac6954d5696bda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
